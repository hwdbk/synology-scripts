# Encrypted multi-disk backup

Simple scripts to generate an index for and copy files to a multi-disk backup disk set. The files are copied to an encrypted file system on the external disk (using eCryptfs) to prevent misuse in case the disk falls into the wrong hands or is disposed of at some point in time. I use it to create and maintain a standalone, offsite backup of my files once or twice a year or so. Now eCryptfs might not be the best possible encryption mechanism to control access to your files, but it's better then losing your files completely through human error or a disaster such as theft or fire on the original NAS. It's all about the balance in risk.

- `backup_diskset_generate` generates a (multi-disk) index/todo list of files to copy. The target disk size is 3TB and fixed (making that flexible would be the first obvious thing to do in extending this script but I didn't get round that yet) but can be changed in the script (the size, that is, not the flexibility of using mixed size disks). Multiple input paths can be specified that will be used for sourcing the files and the script will 'cut' the list into chunks that will (just) fit one disk, and then move on to the next disk. The backup disk set is named after the date (e.g. 20200605); disks are numbered 001, 002 etc. Skipping to the next disk can be forced by using the keyword '`newdisk`' in the input paths.

- `backup_diskset_copy` copies the files as specified by the generated index/todo list to the external disk. The destination path for the files is set in the script to /volumeSATA1/satashare1-1 (which is an external disk connected to the 1st eSATA connector on the Synology NAS), but this can be changed if your backup disks are elsewhere. Options: `mountonly` - will only create and/or mount the eCryptfs file system ; `dryrun` - will only display what will happen ; `nomd5` - will not checksum the copied files.

- `backup_diskset_rename` renames an existing backup disk (or rather the directory on it) to a new backup disk set name. This is useful to make incremental backups and allows existing files to be reused (not rewritten again) after some time if they are already on the backup disk (and haven't changed).

There is a much more efficient alternative to this: `backup_update_generate` and `backup_update_copy`. These scripts start off a file list as created by `mkfilelist_fast` (which is essentially a file scan of a list of directories/shares you want to backup) and a bunch of disks, empty or filled by a previous `backup_diskset_copy` or `backup_update_copy`. The idea is that the file list of each disk is compared against the wanted target file list, and any differences are processed. This means that files that no longer exist in the source file list, will be deleted, new files will be copied and (this is where the efficiency comes in), moved or renamed files are moved/renamed on the backup disks *regardless on which disk they reside*. So, once copied files will not be overwritten or rewritten if they are moved or renamed. Disappeared files (i.e. deleted files on the source) files are deleted on the backup disks and the free space that is returned, will be reused to write the files from the list of new files, effectively filling up the disks again. The algorithm depends on md5sum, which you'll have to compile for your machine (it's in C++).

- `backup_update_generate` generates a (multi-disk) index/todo list of files to copy (`.lst` files). The target disk size is 3TB but can be changed in the script. Parameters are the `.fst` file of the source directories you want to backup (made with `mkfilelist_fast`) and a list of `.fst` files describing the contents of the backup set's individual disks. These files are created as output of `backup_diskset_copy` and `backup_update_copy` - i.e. the results of the previous backup, or you can start with a bunch of empty files if you want to start a whole new backup set. These files should be named `<something>_nnn` where `nnn` is the disk sequence number (001, 002 etc.), as above. I use `YYYYMMDD` for `<something>` because that's the convention here.

- `backup_update_copy` effectuates the change list for the specified disk, as calculated by `backup_update_generate`. Take a look in the `.lst` file for the disk to see what will happen: disappeared files will be deleted, then files will be moved and renamed (even if the file had moved to a different share whose file list was collected on generate), then the remaining/resulting free space is filled up again with new files.

Now there's something to say to both methods: `backup_diskset_generate` will create an ordered disk set and you get to decide which files are written out in which order (on which disks), but updating basically rewrites the entire sequence and pushes data forward across the disks as you data set increases in size. `backup_update_generate`, on the other hand, 100% reuses any file ever written (as long as it is still required in the backup set), but files get moved around and deleted space is reused from any file in the source list, so a directory may get spread across several disks (PS> a file is never split, of course), so after a couple of passes, the disks layout may look a bit messy, but all files are there.

The first method is ideal if you expect to be wanting to restore a specific directory. The latter is ideal to regularly and fastly update the disk set for offline storage and, if push comes to shove, you don't mind popping in a couple of disks to retrieve the contents of a specific directory. You can't have it all...
